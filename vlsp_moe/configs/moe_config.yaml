id: &id 1010042
base_home: &base_home /home/users/sutd/1010042
base_scratch: &base_scratch /scratch/users/sutd/1010042

data:
  max_length: 2048
  num_proc: 2
  validation_subset_size: 10000
  target_path: *base_home/VLSP-2025-MedMT/data/train.vi.txt
  tokenized: *base_home/VLSP-2025-MedMT/data/tokenized
  train_path: *base_home/VLSP-2025-MedMT/data/train.en.txt

dataset:
  en_vi_file: *base_home/VLSP-2025-MedMT/data/processed/en_vi_train.jsonl
  medical_file: *base_home/VLSP-2025-MedMT/data/processed/medical_train.jsonl
  train_file: *base_home/VLSP-2025-MedMT/data/processed/train.jsonl
  val_file: *base_home/VLSP-2025-MedMT/data/processed/val.jsonl
  vi_en_file: *base_home/VLSP-2025-MedMT/data/processed/vi_en_train.jsonl

device: cuda

lora:
  bias: none
  lora_alpha: 16
  lora_dropout: 0.0
  r: 8
  task_type: CAUSAL_LM

model:
  model_id_or_path: Qwen/Qwen3-1.7B
  template: qwen

moe:
  experts:
    - description: Medical domain translation expert
      name: medical
      task_type: medical_translation
    - description: English to Vietnamese translation expert
      name: en_vi
      task_type: en_vi_translation
    - description: Vietnamese to English translation expert
      name: vi_en
      task_type: vi_en_translation
  gating:
    dropout: 0.3
    hidden_size: 256
    load_balancing_weight: 0.01
  num_experts: 3

tokenizer:
  allow_whitespace_only_pieces: true
  bos_id: 2
  character_coverage: 0.995
  eos_id: 3
  input_sentence_size: 10000000
  model_prefix: spm_model
  model_type: unigram
  pad_id: 0
  split_by_number: true
  split_by_unicode_script: true
  split_by_whitespace: true
  split_digits: true
  treat_whitespace_as_suffix: false
  type: sentencepiece
  unk_id: 1
  user_defined_symbols:
    - <|im_start|>
    - <|im_end|>
    - <medical>
    - <en_vi>
    - <vi_en>
  vocab_size: 32000

training:
  seed: 42
  torch_compile: true
  bf16: true
  eval_steps: 6151
  eval_strategy: steps
  fp16: false
  gradient_accumulation_steps: 4
  greater_is_better: True
  label_names:
    - labels
  learning_rate: 2e-4
  lr_scheduler_type: "cosine"
  load_best_model_at_end: true
  logging_dir: *base_scratch/logs/moe_training
  logging_steps: 50
  max_steps: -1
  metric_for_best_model: bleu
  num_train_epochs: 2
  output_dir: *base_scratch/outputs/moe_model
  per_device_eval_batch_size: 1
  per_device_train_batch_size: 20
  gradient_checkpointing: true
  run_name: moe_medmt_run_0108_unsloth
  save_steps: 6151
  save_strategy: steps
  save_total_limit: 5
  warmup_steps: 2460
  weight_decay: 0.05
